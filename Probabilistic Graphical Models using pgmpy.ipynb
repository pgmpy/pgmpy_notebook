{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:3e78a9c08bcee47b27b91f69d94bc25be296c3a0aac03476916dd43796180150"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image, Math"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Machine Learning\n",
      "Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Such algorithms operate by building a model from example inputs and using that to make predictions or decisions, rather than following strictly static program instructions. We will be considering only classification here.\n",
      "\n",
      "There are multiple ways to solve this problem:  \n",
      "1. We could find a function which can directly map an input value to it's class label. \n",
      "2. We can find the probability distributions over the variables and then use this distribution to answer queries about the new data point.\n",
      "\n",
      "In Probabilistic Graphical Models we try to do predictions using the second method. \n",
      "The most obvious way to do this would be to compute a Joint Probability Distribution over all these variables and then marginalize and reduce over these according to our new data point to get the probabilities of classes.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's take an example of class where we want to predict things about students. So, we have some previous data of students and for each student we have 5 features: Difficulty (D), Intelligence (I), SAT (S), Grade (G) and Recommendation Letter (L). For simplicity, we will also consider that each of these features can have only two values. So, difficulty can take two values easy and hard, Intelligence can take smart and dumb and so on.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's now generate some random data. \n",
      "# Now let's try to compute the Joint Probability Distribution over these variables."
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We now have a Joint Distribution $ P(D, I, S, G, L) $. Now let's say we have a new student and we want to predict what recommendation letter will he get and also say that we know that he is intelligent, the course was easy, his SAT score is good, and also he got a good grade. So, basically we want to compute $ P(L | d^0, i^1, s^1, g^1) $. From chain rule of probability we know that:\n",
      "$ P(D, I, S, G, L) = P(L | D, I, S, G) * P(D, I, S, G) $\n",
      "$ P(D, I, S, G, L) = P(D | I, S, G, L) * P(I | S, G, L) * P(S | G, L) * P(G | L) * P(L) $\n",
      "$ P(L | d^0, i^0, s^1, g^1) = \\frac {P(d^0, i^1, s^1, g^1, L)} {P(d^0, i^1, s^1, g^1)} $\n",
      "\n",
      "Since we know the joint distribution, $ P(D, I, S, G, L) $ we can now easily compute both the terms $ P(d^0, i^1, s^1, g^1, L) $ and $ P(d^0, i^1, s^1, g^1) $ and using these we will be able to compute the $ P(L | d^0, i^0, s^1, g^1) $.\n",
      "\n",
      "#### TODO: Show the computed values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The way we solved the problem above we had to compute the whole joint distribution to answer queries over this model. And we can see that the size of the table is exponential to the number of variables involved. So, in the case of large models this method becomes infeasible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Probabilistic Graphical Models (PGM)\n",
      "\n",
      "Probabilistic Graphical Model is a way of compactly representing Joint Probability distribution over random variables by exploiting the independence conditions of the variables. And PGM also provides us methods to easly and efficiently compute conditional probabilities over joint distributions. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### How is PGM different than other techniques?\n",
      "\n",
      "The distinctive thing about PGM is that it provides a very intuitive and natural approach for modelling complex problems along with maintaining control over the computational costs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "There are two main types of modes in PGM:\n",
      "1. Bayesian Models: Bayesian Models are used mainly when we have causal relationship between the variables\n",
      "2. Markov Models: When non causal relationship."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In the student example that we saw, we mostly have a causal relationship between variables. So, if a student is intelligent it implies that he should get good grades, getting good grades implies that he will get a better recommendation. Let's try to create a graph of dependencies."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"files/student.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We used our intuition on mostly local parts of the network but using the properties of Bayesian Networks we can also see global independencies. Connections between variables in a directed graph can only be the following four ways:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/connections_directed_graphs.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of causal, evidential and common cause connections A and C are dependent if B is not observed but if B is observed then the become independent. Whereas in the case of common evidence A and C are independent if B is not observed but become dependent if B is observed.\n",
      "\n",
      "So in the case of our student example we can now get many dependence assertions over the network like $ L \\perp D | G $, $ L \\perp I | G $, $ D \\perp I $, $ D \\not\\perp I | G $ etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now coming back to our original joint distribution distribution:\n",
      "$$ P(D, I, G, S, R) = P(D | I, G, S, R) * P(I | G, S, R) * P(G | S, R) * P(S | R) * P(R) $$\n",
      "\n",
      "So using these independence properties in a Bayesian Network we can reduce this to:\n",
      "$$ P(D, I, G, S, R) = P(D) * P(I) * P(G | D, I) * P(S | I) * P(L | G) $$\n",
      "\n",
      "And we can also notice that all the terms in this are the probabilites of each the variables given their parents in the network. These terms are known as the conditional Probability distributions of the variables. So basically to compute the Joint Distribuiton we can compute the product of CPDs of all the variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Another example using pgmpy \n",
      "\n",
      "Let's see an example for predicting the price of a house. For simplicity we will consider that the price of the house depends only on Area, Location, Furnishing, Crime Rate and Distance from the airport. And also we will consider that all of these are discrete variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Our raw data would look something like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "raw_data = np.random.randint(low=0, high=2, size=(1000, 6))\n",
      "print(raw_data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 1 1 0]\n",
        " [0 0 0 0 1 0]\n",
        " [1 1 1 0 0 0]\n",
        " ..., \n",
        " [1 1 0 1 1 0]\n",
        " [0 1 0 1 0 1]\n",
        " [1 1 1 0 0 1]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Probabilistic Graphical Models keeps the representation of Joint Distribution simple by exploiting the dependencies in the variables. And these dependencies are represented using a graph. Now let's create a model for the interaction of our variables using intuition:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/housing_price_small.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/housing_price.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "For each node/variable in the network we assign a conditional probability distribution to it. The conditional probability distribution represents the probability of the variable when its parents are observed. So now using our raw data let's assign CPDs to each of these nodes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pgmpy.models import BayesianModel\n",
      "import pandas as pd\n",
      "data = pd.DataFrame(raw_data, columns=['A', 'C', 'D', 'L', 'F', 'P'])\n",
      "data_train = data[: int(data.shape[0] * 0.75)]\n",
      "model = BayesianModel([('F', 'P'), ('A', 'P'), ('L', 'P'), ('C', 'L'), ('D', 'L')])\n",
      "model.fit(data_train)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### What does fit does ?\n",
      "The fit method adds a Conditional Probability Distribution (CPD) to each of the node in our model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src='files/housing_price_with_CPD.png'>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.get_cpds()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "[<pgmpy.factors.CPD.TabularCPD at 0x7fee0f708c50>,\n",
      " <pgmpy.factors.CPD.TabularCPD at 0x7fee1993e8d0>,\n",
      " <pgmpy.factors.CPD.TabularCPD at 0x7fee0f708cf8>,\n",
      " <pgmpy.factors.CPD.TabularCPD at 0x7fee0f4a0470>,\n",
      " <pgmpy.factors.CPD.TabularCPD at 0x7fee0f4a0550>,\n",
      " <pgmpy.factors.CPD.TabularCPD at 0x7fee0f4a0cf8>]\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.get_cpds('P')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "<pgmpy.factors.CPD.TabularCPD at 0x7fee1993e8d0>"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But the data that we have for training might be baised so with pgmpy we also have the option to assign your own Conditional Probability Distributions. Let's say the probability of getting an unfurnished home is equal to getting a furnished house. Let's adjust the values according to this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pgmpy.factors import TabularCPD\n",
      "f_cpd = TabularCPD('F', 2, [[0.5], [0.5]])\n",
      "\n",
      "model.remove_cpd('F')\n",
      "model.add_cpd(f_cpd)\n",
      "\n",
      "model.check_model()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "True"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Now let's try to do some reasoning on our model to verify if our intuitution for the model was correct or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pgmpy.Inference import VariableElimination\n",
      "model = VariableElimination(model)\n",
      "# Returns a probability distribution over variables A and B.\n",
      "model.query(variables=['A', 'B'])\n",
      "print(model.query(variables=['P'], conditions={'L': {0}, 'F': {0}, 'A': {0}}))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "P          phi(A)  \n",
      "------------------\n",
      "P_0        0.96\n",
      "P_1        0.04\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If you think about prediction about new values from this model, it is basically the same what we have been doing here. We basically ask questions about the probability of some variable giving conditions for other variables. Also we can account for missing values with just leaving it blank."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data = data[0.75 * data.shape[0] : data.shape[0]]\n",
      "test_data.drop('P', axis=1, inplace=True)\n",
      "model.predict(test_data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Why to use PGM rather than simply computing these values from the probaility distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The graph structure implies some independence conditions over the variables. The variables can be indirectly connected to each other in the following ways:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src='files/connection.png'>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This independence due to the structure is responsible for the reduced computational complexity for inference.\n",
      "\n",
      "For the Joint Probability distribution over all the variables we can write it as:\n",
      "\n",
      "$$ P(A, C, D, L, F, P) = P(A) * P(C | A) * P(D | A, C) * P(L | A, C, D) * P(F | A, C, D, L) * P(P | A, C, D, L, F) $$\n",
      "\n",
      "But if we apply all the independency conditions that we saw above in this equation we get:\n",
      "\n",
      "$$ P(A, C, D, L, F, P) = P(A) * P(L | C, D) * P(C) * P(D) * P(P | F, A, L) * P(F)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "For doing inference over this model we can simply eliminate variables or condition this joint probability.\n",
      "\n",
      "Say if we want to calcualate $ P(A) $ we could simply calculate:\n",
      "\n",
      "$$P(A) =  \\sum_{C} \\sum_{D} \\sum_{L} \\sum_{F} \\sum_{P} P(A, C, D, L, F, P) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "This algorithm of summing over variables that are not required is known as Variable Elimination."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### How to construct model from the data?\n",
      "\n",
      "Doing inference from the model is really simple. But the tough part is to create the model from the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The house price estimation example had very intuitive variables and thus we were able to construct the model very easily. But this is not always the case.\n",
      "\n",
      "So for constructing the model we use the independence properties implied by the model and by the Joint Probability distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "One of the simplest way to construct a model is to find out some independence conditions in the data and according to that we can use that to arrange our variables in the graph structure in such a way to satisfy those independency conditions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Also local independencies make it simpler to construct models. Give example of local independencies in the case of Bayesian Models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "These were the most basic ways for constructing models from distributions. There are many more ways of finding structure in data to properly model them like density estimation etc."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}